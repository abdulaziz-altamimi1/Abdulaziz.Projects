{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef82251",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraires\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras import layers, callbacks\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b8e1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read csv file\n",
    "df = pd.read_csv('walmart_data.csv')\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caeb93e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check dataset\n",
    "print(\"The Shape:\",df.shape)\n",
    "print(\"The NULL values:\\n\",df.isnull().sum())\n",
    "print(\"Number of duplicated values\",df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaca9e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop outliers\n",
    "q1 = df['Weekly_Sales'].quantile(0.25)\n",
    "q3 = df['Weekly_Sales'].quantile(0.75)\n",
    "IQR = q3 - q1\n",
    "lower = q1 - 1.5 * IQR\n",
    "upper = q3 + 1.5 * IQR\n",
    "df = df[(df['Weekly_Sales'] >= lower) & (df['Weekly_Sales'] <= upper)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf2b26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#descriptive statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4af9efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary of a DataFrame\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4dec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply Label encoder\n",
    "df_categorical = df.select_dtypes(include='object')\n",
    "encoder = LabelEncoder()\n",
    "df_categorical = df_categorical.apply(encoder.fit_transform)\n",
    "df.drop(df_categorical.columns, axis=1, inplace=True)\n",
    "df = pd.concat([df, df_categorical], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6d76e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target variable\n",
    "features = [\n",
    "    'Store', 'Dept', 'MarkDown1', 'MarkDown2', 'MarkDown3',\n",
    "    'MarkDown4', 'MarkDown5', 'Type', 'Size',\n",
    "    'IsHoliday', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9528380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only positive Weekly Sales \n",
    "df = df[df['Weekly_Sales'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56df067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target variable\n",
    "X= df.drop('Weekly_Sales',axis=1)\n",
    "y = np.log1p(df['Weekly_Sales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce98a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d010dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7065885f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build network model\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(512, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51d1fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1529b3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply early stopping \n",
    "early_stop = callbacks.EarlyStopping(\n",
    "    patience=15,\n",
    "    min_delta=0.0001,\n",
    "    restore_best_weights=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a84db19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=300,\n",
    "    batch_size=1024,\n",
    "    callbacks=[early_stop],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4f6b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare the model loss against the actual val_loss\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df[['loss', 'val_loss']].plot(title=\"Mean Squared Error\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "#compare the model mae against the actual val mae\n",
    "history_df[['mae', 'val_mae']].plot(title=\"Mean Absolute Error\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d736ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance metrics\n",
    "y_pred = model.predict(X_test_scaled).flatten()\n",
    "y_pred_actual = np.expm1(y_pred)\n",
    "y_test_actual = np.expm1(y_test)\n",
    "\n",
    "rmse = metrics.root_mean_squared_error(y_test_actual, y_pred_actual)\n",
    "r2 = metrics.r2_score(y_test_actual, y_pred_actual)\n",
    "mae = metrics.mean_absolute_error(y_test_actual, y_pred_actual)\n",
    "\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0851a0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance metrics with Log\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "rmse = metrics.root_mean_squared_error(y_test, y_pred)\n",
    "r2 = metrics.r2_score(y_test, y_pred)\n",
    "mae = metrics.mean_absolute_error(y_test,y_pred)\n",
    "\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79907fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test evaluate\n",
    "test_loss, test_mae = model.evaluate(X_test_scaled, y_test)\n",
    "print(f\"Test RMSE: {np.sqrt(test_loss):.4f}\")\n",
    "print(f\"Test MAE: {test_mae:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
